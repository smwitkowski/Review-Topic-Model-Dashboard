---
title: "02-preparing-text-for-modeling"
output: html_document
date: '2022-06-14'
---


```{r Load libraries}
library(dplyr)
library(ggplot2)
library(lubridate)
library(googleCloudStorageR)
library(gargle)

options(gargle_oauth_email = TRUE)
scope <- c("https://www.googleapis.com/auth/cloud-platform")
token <- token_fetch(scopes = scope)
gcs_auth(token = token)
gcs_global_bucket("development-bucket-0622")

theme_set(theme_minimal())
```

## Introduction

### Loading our data

We're going to working with a set of restaurant reviews from TripAdvisor which was found and downloaded from [Kaggle](https://www.kaggle.com/datasets/inigolopezrioboo/a-tripadvisor-dataset-for-nlp-tasks). There are six different datasets, each containing reviews for restaurants from different cities. While we're going to use each of them to build the final Shiny app, in this Markdown we'll just be walking through reviews from New York City.

```{r Read in reviews}
reviews <-
  gcs_get_object('topic-model-dashboard/data/raw/New_York_reviews.csv')

glimpse(reviews)
```

The authors of the dataset give the following description of the features:

> -   parse_count: numerical (integer), corresponding number of extracted review by the web scraper (auto-incremental)
> -   author_id: categorical (string), univocal, incremental and anonymous identifier of the user (UID_XXXXXXXXXX)
> -   restaurant_name: categorical (string), name of the restaurant matching the review
> -   rating_review: numerical (integer), review score in the range 1-5
> -   sample: categorical (string), indicating "positive" sample for scores 4-5 and "negative" for scores 1-3
> -   review_id: categorical (string), univocal and internal identifier of the review (review_XXXXXXXXX)
> -   title_review: text, review title
> -   review_preview: text, preview of the review, truncated in the website when the text is very long
> -   review_full: text, complete review
> -   date: timestamp, publication date of the review in the format (day, month, year)
> -   city: categorical (string), city of the restaurant which the review was written for
> -   url_restaurant: text, restaurant url

We won't be needing all of the data made available here. Below we explicitly call out which columns we want to keep and what order we want them to appear in the dataset.

```{r Subset columns}
keep_cols <- c(
  'review_id',
  'author_id',
  'date',
  'restaurant_name',
  'title_review',
  'review_full',
  'rating_review',
  'sample'
)

reviews <- reviews[keep_cols]

glimpse(reviews)
```

### Cleaning our data

It seems that some of the data has not been read in the appropriate format. For example, `review_id` is a string even though it should be a categorical variable (otherwise known as a factor in R). Or take `date`; it's been read is as a string as well, but it should be a date.

Before moving forward, we'll need to convert each variable to the correct data type as needed.

```{r Convert review_date to date}
reviews$date <- as_date(reviews$date, format = '%B %d, %Y')

reviews$review_id <- factor(reviews$review_id)
reviews$author_id <- factor(reviews$author_id)
reviews$restaurant_name <- factor(reviews$restaurant_name)
reviews$sample <- factor(reviews$sample)

glimpse(reviews)
```

You may also notice in that the restaurant name has an underscore in it's name. This may be an artifact of the data collection that spans across all data points. Let's dig a bit deeper to understand what the names look like.

```{r Show some restaurant names}
unique(reviews$restaurant_name)[1:25]
```

It does seem like there are no spaces in the restaurants name. While that might have been necessary for some other purpose, we'd like to see the restaurant name look as readable as possible for our final dashboard. With that in mind, we'll clean up this a bit.

Now, we can't just remove the underscored and replace them with spaces. The 28th item in this list contains "Masto_s\_Steakhouse"; apostrophes need to be added in certain cases.

```{r Remove the underscores}
restaurant_names <- unique(reviews$restaurant_name)
restaurant_names_new <-
  gsub('(_(?=(s|t)(_|$)))', "'", restaurant_names, perl = TRUE)

restaurant_names_new <-
  gsub('(?<=(^[A-Z]{1}))_(?=[A-Z]{1}_)',
       ".",
       restaurant_names_new,
       perl = TRUE)
restaurant_names_new <-
  gsub('(?<=(^[A-Z]{1}\\.[A-Z]{1}))_',
       ". ",
       restaurant_names_new,
       perl = TRUE)

restaurant_names_new <-
  gsub('((?<=(O|L|D|I))_)', "'", restaurant_names_new, perl = TRUE)
restaurant_names_new <-
  gsub('_', " ", restaurant_names_new, perl = TRUE)


updated_indicies <-
  grep(
    '(?<=(^[A-Z]{1}))_(?=[A-Z]{1}_)|(_(?=(s|t)(_|$)))|((?<=(O|L|D|I))_)',
    restaurant_names,
    perl = TRUE
  )
sample_indicies <-
  sample(updated_indicies, size = 5, replace = FALSE)

for (i in sample_indicies) {
  cat(
    sprintf(
      "Original name: %s \nNew Name: %s \n %s \n",
      restaurant_names[i],
      restaurant_names_new[i],
      paste(rep('=', 15), collapse = "")
    )
  )
}
```

While it's not perfect and there are likely some names that aren't captured, it is a great deal more readable than before. Let's map these values to a new column. We're going to rename the original columns and make the "readable" populate the `restraunt_name` column.

```{r Name fixing function}

reviews$restaurant_name_original <- reviews$restaurant_name

reviews$restaurant_name <- plyr::mapvalues(reviews$restaurant_name_original,
                                     from = restaurant_names,
                                     to = restaurant_names_new)

```

## Exploring our Data

### How have reviews changed over time?

First I'd like to see how the volume of these reviews have changed overtime. I notice that the first three reviews were submitted in 2020, which was during many of the COVID lockdowns. I expect to see a large dip in that time frame, and that the appearance of those reviews in the top of the dataset is just a coincidence.

```{r Plotting reviews over time}
reviews %>%
  count(date) %>%
  ggplot(., aes(x = date, y = n)) +
  geom_line() +
  labs(x = 'Review Date',
       y = 'Number of Reviews',
       title = 'Count of Reviews Over Time') +
  theme(plot.title = element_text(hjust = 0.5))

```

As expected, most of the reviews come before the 2020 lockdowns. If we were trying to provide semi-real time feedback to restaurants on what their customers were saying, this would pose as a serious data issue. We may start to consider other data sources to append onto this in order to get better review coverage.

However for this exercise it's perfectly fine that the reviews slow down in 2020, so we will move forward.

### Which restaurants are recieving reviews?

We notice in many other industries that there are a few categories that dominate an entire group. Is that the case here? Are reviews "top-heavy" where only a handful or restaurants get the majority of reviews?

```{r Distribution of reviews}
reviews %>%
  count(restaurant_name) %>%
  mutate(perc = n / sum(n)) %>%
  filter(rank(desc(n)) <= 25) %>%
  arrange(desc(n)) %>%
  ggplot(., aes(y = reorder(restaurant_name, perc), x = perc)) +
  geom_bar(stat = 'identity') +
  scale_x_continuous(labels = scales::percent) +
  labs(x = 'Percentage of Total Reviews',
       y = 'Restaurant Name',
       title = 'Top 25 Restaurants by Review Share') +
  theme(plot.title = element_text(hjust = 0.5))
```

It seems that there are a lot of *really* popular restaurants that get a significant portion of the reviews. However, after looking at this chart it doesn't seem like the reviews are dominated by a handful of restaurants.

To get a better view of this, let's build a cumulative distribution chart. This chart should give us a better idea of how well distributed the reviews are. A perfectly distributed set of reviews would yield a straight line.

```{r CDF of restaurant reviews}
plt_data <- reviews %>%
  count(restaurant_name) %>%
  mutate(perc = n / sum(n)) %>%
  arrange(desc(perc)) %>%
  mutate(
    cdf = cumsum(perc),
    cdf_order = row_number(),
    cdf_order_perc = cdf_order / max(cdf_order)
  )

ind_80 <- which(plt_data$cdf >= .8)[1]
y_end <- plt_data$cdf[ind_80]
x_end <- plt_data$cdf_order_perc[ind_80]

ggplot(plt_data, aes(x = cdf_order_perc, y = cdf)) +
  geom_line() +
  geom_segment(
    x = x_end,
    xend = x_end,
    y = -Inf,
    yend = y_end,
    color = 'red',
    size = 0.2
  ) +
  geom_segment(
    x = -Inf,
    xend = x_end,
    y = y_end,
    yend = y_end,
    color = 'red',
    size = 0.2
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = 'Percentage of Total Restaurants',
    y = 'Percentage of Total Reviews',
    title = 'Cumulative Distribution of Restraunt Reviews',
    caption = sprintf(
      '%s of restaurants recieved %s of all reviews',
      scales::label_percent()(x_end),
      scales::label_percent()(y_end)
    )
  ) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))

remove(plt_data, ind_80, x_end, y_end)
```

So the reviews aren't perfect distributed, which is to be expected. Yet, there is a decent distribution in reviews. Roughly 30% of restaurants reviewed 80% of the reviews. This follows our expectations: there are some popular restaurants, and a lot of not-so-popular restaurants.

It'll be important to consider how many reviews each restaurant has received when we go to build a topic model. If the number of reviews doesn't meet a certain threshold we won't be able to build a topic model (nor should we if there are only 20 reviews, for example.)

### Who is submitting reviews?

Similar to how we considered how many reviews were submitted for restaurants, we ought to consider how many reviews are being submitted. I imagine there may be some TripAdvisor "power users" who submit *tons* of reviews.

Let's first look at the total distribution of the review count by author.

```{r Who is submitting reviews}
reviews %>%
  count(author_id) %>%
  ggplot(., aes(x = n)) +
  geom_histogram() +
  scale_y_continuous(label = scales::comma) +
  labs(x = 'Total Number of Reviews Submitted',
       y = 'Number of Authors',
       title = 'Distribution of Total Reviews Submitted') +
  theme(plot.title = element_text(hjust = 0.5))
```

This appears to be a poisson distribution, which is common among counts. Most of the authors have submitted fewer than 50 reviews. However, the x-axis extends to 200, which leads me to believe there may be some authors with hundred of reviews. Let's take a look at the authors with the most reviews submitted.

```{r Top authors by count}
reviews %>%
  count(author_id) %>%
  mutate(perc = n / sum(n)) %>%
  filter(rank(desc(n)) <= 25) %>%
  arrange(desc(n)) %>%
  ggplot(., aes(y = reorder(author_id, perc), x = n)) +
  geom_bar(stat = 'identity') +
  scale_x_continuous(labels = scales::comma) +
  labs(x = 'Count Total Reviews',
       y = 'Author ID',
       title = 'Top 25 Review Authors by Count') +
  theme(plot.title = element_text(hjust = 0.5))
```

At the top of our list is `UIS_7760` who has submitted over 250 reviews! Now, there's a chance these aren't legitimate reviews. It may be a bot programmed to submit reviews, or even a review company. We're not going to investigate that here, but high volume accounts should always be looked at with a little bit of skepticism.

To get a better idea on how well distributed the reviews are, let's build another cumulative distribution chart.

```{r CDF of author reviews}
plt_data <- reviews %>%
  count(author_id) %>%
  mutate(perc = n / sum(n)) %>%
  arrange(desc(perc)) %>%
  mutate(
    cdf = cumsum(perc),
    cdf_order = row_number(),
    cdf_order_perc = cdf_order / max(cdf_order)
  )

ind_80 <- which(plt_data$cdf >= .8)[1]
y_end <- plt_data$cdf[ind_80]
x_end <- plt_data$cdf_order_perc[ind_80]

ggplot(plt_data, aes(x = cdf_order_perc, y = cdf)) +
  geom_line() +
  geom_segment(
    x = x_end,
    xend = x_end,
    y = -Inf,
    yend = y_end,
    color = 'red',
    size = 0.2
  ) +
  geom_segment(
    x = -Inf,
    xend = x_end,
    y = y_end,
    yend = y_end,
    color = 'red',
    size = 0.2
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = 'Percentage of Total Restaurants',
    y = 'Percentage of Total Reviews',
    title = 'Cumulative Distribution of Restraunt Reviews',
    caption = sprintf(
      '%s of authors submitted %s of all reviews',
      scales::label_percent()(x_end),
      scales::label_percent()(y_end)
    )
  ) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))

remove(plt_data, ind_80, x_end, y_end)
```

These reviews are much more "equally" distributed, but we still see some imbalances. It seems that a small percentage of authors, roughly seven or eight percent, make up 25% of the reviews. Knowing that all reviews aren't created equal either (some accounts are given "boosts" or priority when being sorted), that may cause some concern regarding the integrity or coverage of the reviews a user is seeing.

However, almost 60% of authors make up 80% of reviews. That should give us confidence that the true attributed will rise to the top.

### How is the score distributed?

Along with the review, users provide a score on a scale from one to five. This scale is common across industries, and the average score can give a good idea on how "good" a restaurant is.

Let's first take a look at how the scores are distributed.

```{r Average reviews}
ggplot(reviews) +
  geom_bar(aes(rating_review)) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    x = 'Rating',
    y = 'Count of Reviews',
    title = 'Number of Reviews Submitted by Rating',
    caption = 'Most reviews receive a score of four or five'
  ) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))

```

It's evident that most people submit a rating of five. That doesn't seem out of the ordinary, if everything is as expected, I imagine users will "default" to a five since they find no fault.

Let's take a look and see how the average ratings across users is distributed.

```{r Distribution of average user review}
reviews %>%
  group_by(author_id) %>%
  filter(n() >= 15) %>%
  summarise(average_score_given = mean(rating_review)) %>%
  ggplot(., aes(average_score_given)) +
  geom_density()
```

However, there might be some users who are more scrupulous in their ratings. To give an anecdote, I had a professor in college who carried a grading philosophy where only exceptional work was given an A. If you met all the requirements, you got a B or a C. A's were reserved only for the best of the best work. This undoubtedly made that class a bit harder for myself, but it could also make comparing grades across professors a little difficult, seeing that their grading criteria was different.

Let's take a look at our users, and how their average ratings vary from the global average. For a more accurate look at the data, we're only going to consider users with five or more.

```{r Scatterplot of average review by review count}
reviews %>%
  mutate(average_score = mean(rating_review)) %>%
  group_by(author_id) %>%
  filter(n() >= 5) %>%
  summarise(
    average_score_given = mean(rating_review),
    number_of_reviews = n(),
    average_score_delta = mean(rating_review) - mean(average_score)
  ) %>%
  ggplot(., aes(number_of_reviews, average_score_delta)) +
  geom_point(size = 0.1) +
  geom_smooth() +
  labs(
    x = 'Number of Reviews',
    y = 'Average Rating Delta',
    title = 'Average Review Delta by Review Count',
    caption = 'There is a slight negative correlation between the number of reviews and average review'
  ) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))


```

We see a slightly negative correlation, which intuitively makes sense. When you review more restaurants you likely become more aware of below-par service.

## Conclusion

In this notebook, we took a look at our data and did some simple cleaning. In the next notebook we'll dive deeper into the actual reviews and prepare that text data to be modeled. 

Before we go, let's save our data to GCS for future use.

```{r Save data to GCS}
f <- function(input, output){
  write.csv(input, file = output, row.names = FALSE)
}
gcs_upload_set_limit(2000000000L)
gcs_upload(file = reviews, 
           name = 'topic-model-dashboard/data/interim/New_York_reviews.csv',
           object_function = f,
           predefinedAcl = "bucketLevel")
```
